% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/provider-bedrock.R
\name{chat_bedrock}
\alias{chat_bedrock}
\title{Chat with an AWS bedrock model}
\usage{
chat_bedrock(
  system_prompt = NULL,
  turns = NULL,
  model = NULL,
  profile = NULL,
  api_args = list(),
  echo = NULL
)
}
\arguments{
\item{system_prompt}{A system prompt to set the behavior of the assistant.}

\item{turns}{A list of \link{Turn}s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.}

\item{model}{The model to use for the chat. The default, \code{NULL}, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.}

\item{profile}{AWS profile to use.}

\item{api_args}{Named list of arbitrary extra arguments appended to the body
of every chat API call. Valid arguments are:
\itemize{
\item \code{max_tokens}: Maximum number of tokens to generate before stopping.
\item \code{temperature}: Controls the randomness of the output. Higher values
produce more random and creative responses. Range [0, 1].
\item \code{top_p}: Controls the diversity of the output. Higher values produce
more diverse responses. Range [0, 1]. Temperature is typically
preferred.
\item \code{top_k}: Controls the number of possible next tokens to consider for
the next token in the output. Higher values produce more diverse
responses. A positive integer. Temperature is typically preferred.
\item \code{stop_sequences}: A list of strings to stop the model from generating
more tokens.
}}

\item{echo}{One of the following options:
\itemize{
\item \code{none}: don't emit any output (default when running in a function).
\item \code{text}: echo text output as it streams in (default when running at
the console).
\item \code{all}: echo all input and output.
}

Note this only affects the \code{chat()} method.}
}
\value{
A \link{Chat} object.
}
\description{
\href{https://aws.amazon.com/bedrock/}{AWS Bedrock} provides a number of
language models, including those from Anthropic's
\href{https://aws.amazon.com/bedrock/claude/}{Claude}, using the Bedrock
\href{https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html}{Converse API}.
Although Ellmer provides a default model, you'll need to
specify a model that you actually have access to using the \code{model} argument.
If using \href{https://aws.amazon.com/blogs/machine-learning/getting-started-with-cross-region-inference-in-amazon-bedrock/}{cross-region inference},
you'll need to use the inference profile ID for
any model argument, e.g., \code{model="us.anthropic.claude-3-5-sonnet-20240620-v1:0"}.
For examples of tool usage, asynchronous input, and other advanced features,
visit the \href{https://posit-dev.github.io/ellmer/vignettes/}{vignettes} section
of the repo.
\subsection{Authentication}{

Authentication is handled through \{paws.common\}, so if authentication
does not work for you automatically, you'll need to follow the advice
at \url{https://www.paws-r-sdk.com/#credentials}. In particular, if your
org uses AWS SSO, you'll need to run \verb{aws sso login} at the terminal.
}
}
\examples{
\dontrun{
# Basic usage
chat <- chat_bedrock()
chat$chat("Tell me three jokes about statisticians")
}
}
\seealso{
Other chatbots: 
\code{\link{chat_claude}()},
\code{\link{chat_cortex_analyst}()},
\code{\link{chat_databricks}()},
\code{\link{chat_deepseek}()},
\code{\link{chat_gemini}()},
\code{\link{chat_github}()},
\code{\link{chat_groq}()},
\code{\link{chat_ollama}()},
\code{\link{chat_openai}()},
\code{\link{chat_openrouter}()},
\code{\link{chat_perplexity}()}
}
\concept{chatbots}
